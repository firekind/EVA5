{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599906793823",
   "display_name": "Python 3.7.9 64-bit ('eva5': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting git+https://github.com/firekind/athena\n  Cloning https://github.com/firekind/athena to c:\\users\\shyam\\appdata\\local\\temp\\pip-req-build-rm7na0lh\nRequirement already satisfied (use --upgrade to upgrade): athena==0.0.1 from git+https://github.com/firekind/athena in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages\nRequirement already satisfied: pkbar==0.4 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from athena==0.0.1) (0.4)\nRequirement already satisfied: torchsummary in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from athena==0.0.1) (1.5.1)\nRequirement already satisfied: tqdm in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from athena==0.0.1) (4.48.2)\nRequirement already satisfied: matplotlib in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from athena==0.0.1) (3.3.1)\nRequirement already satisfied: numpy in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from pkbar==0.4->athena==0.0.1) (1.19.1)\nRequirement already satisfied: python-dateutil>=2.1 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (2.8.1)\nRequirement already satisfied: pillow>=6.2.0 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (7.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (1.2.0)\nRequirement already satisfied: certifi>=2020.06.20 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from matplotlib->athena==0.0.1) (2020.6.20)\nRequirement already satisfied: six>=1.5 in d:\\users\\shyam\\scoop\\apps\\miniconda3\\current\\envs\\eva5\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->athena==0.0.1) (1.15.0)\nBuilding wheels for collected packages: athena\n  Building wheel for athena (setup.py): started\n  Building wheel for athena (setup.py): finished with status 'done'\n  Created wheel for athena: filename=athena-0.0.1-py3-none-any.whl size=17745 sha256=fa522e2228befd5a7091e40c47cc44c23b4c1c8d5d88b02acbeff7b19fb37af8\n  Stored in directory: C:\\Users\\shyam\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-wmxwqraz\\wheels\\c2\\36\\ea\\fe5a118d0035f6f760fc49471824263f1b0e611bcba3555bde\nSuccessfully built athena\n"
    }
   ],
   "source": [
    "! pip install git+https://github.com/firekind/athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "import torchvision as tv\n",
    "\n",
    "from athena import ClassificationSolver, Experiment, datasets\n",
    "from athena.layers import GhostBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128 if torch.cuda.is_available() else 64\n",
    "# batch_size = 4\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super(DepthwiseConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding=padding, groups=in_channels)\n",
    "        self.point = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.point(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SirNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SirNet, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv1 = DepthwiseConv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv2 = DepthwiseConv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels = 3, dropout_value = 0.25):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            DepthwiseConv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "            \n",
    "            DepthwiseConv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "\n",
    "            DepthwiseConv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "        )\n",
    "\n",
    "        self.transition1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 32, 1),\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            DepthwiseConv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "            \n",
    "            DepthwiseConv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "\n",
    "            DepthwiseConv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "        )\n",
    "\n",
    "        self.transition2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 64, 1),\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            DepthwiseConv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "            \n",
    "            DepthwiseConv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "\n",
    "            DepthwiseConv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_value),\n",
    "        )\n",
    "\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.AvgPool2d(7),\n",
    "            nn.Conv2d(256, 64, 1),\n",
    "            nn.Conv2d(64, 10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.transition1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.transition2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.out_block(x)\n",
    "\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 3, 32, 32]              30\n            Conv2d-2           [-1, 32, 32, 32]             128\n   DepthwiseConv2d-3           [-1, 32, 32, 32]               0\n       BatchNorm2d-4           [-1, 32, 32, 32]              64\n              ReLU-5           [-1, 32, 32, 32]               0\n           Dropout-6           [-1, 32, 32, 32]               0\n            Conv2d-7           [-1, 32, 32, 32]             320\n            Conv2d-8           [-1, 64, 32, 32]           2,112\n   DepthwiseConv2d-9           [-1, 64, 32, 32]               0\n      BatchNorm2d-10           [-1, 64, 32, 32]             128\n             ReLU-11           [-1, 64, 32, 32]               0\n          Dropout-12           [-1, 64, 32, 32]               0\n           Conv2d-13           [-1, 64, 32, 32]             640\n           Conv2d-14          [-1, 128, 32, 32]           8,320\n  DepthwiseConv2d-15          [-1, 128, 32, 32]               0\n      BatchNorm2d-16          [-1, 128, 32, 32]             256\n             ReLU-17          [-1, 128, 32, 32]               0\n          Dropout-18          [-1, 128, 32, 32]               0\n        MaxPool2d-19          [-1, 128, 16, 16]               0\n           Conv2d-20           [-1, 32, 16, 16]           4,128\n           Conv2d-21           [-1, 32, 16, 16]             320\n           Conv2d-22           [-1, 64, 16, 16]           2,112\n  DepthwiseConv2d-23           [-1, 64, 16, 16]               0\n      BatchNorm2d-24           [-1, 64, 16, 16]             128\n             ReLU-25           [-1, 64, 16, 16]               0\n          Dropout-26           [-1, 64, 16, 16]               0\n           Conv2d-27           [-1, 64, 16, 16]             640\n           Conv2d-28          [-1, 128, 16, 16]           8,320\n  DepthwiseConv2d-29          [-1, 128, 16, 16]               0\n      BatchNorm2d-30          [-1, 128, 16, 16]             256\n             ReLU-31          [-1, 128, 16, 16]               0\n          Dropout-32          [-1, 128, 16, 16]               0\n           Conv2d-33          [-1, 128, 16, 16]           1,280\n           Conv2d-34          [-1, 256, 16, 16]          33,024\n  DepthwiseConv2d-35          [-1, 256, 16, 16]               0\n      BatchNorm2d-36          [-1, 256, 16, 16]             512\n             ReLU-37          [-1, 256, 16, 16]               0\n          Dropout-38          [-1, 256, 16, 16]               0\n        MaxPool2d-39            [-1, 256, 8, 8]               0\n           Conv2d-40             [-1, 64, 8, 8]          16,448\n           Conv2d-41             [-1, 64, 8, 8]             640\n           Conv2d-42            [-1, 128, 8, 8]           8,320\n  DepthwiseConv2d-43            [-1, 128, 8, 8]               0\n      BatchNorm2d-44            [-1, 128, 8, 8]             256\n             ReLU-45            [-1, 128, 8, 8]               0\n          Dropout-46            [-1, 128, 8, 8]               0\n           Conv2d-47            [-1, 128, 8, 8]           1,280\n           Conv2d-48            [-1, 256, 8, 8]          33,024\n  DepthwiseConv2d-49            [-1, 256, 8, 8]               0\n      BatchNorm2d-50            [-1, 256, 8, 8]             512\n             ReLU-51            [-1, 256, 8, 8]               0\n          Dropout-52            [-1, 256, 8, 8]               0\n           Conv2d-53            [-1, 256, 8, 8]           2,560\n           Conv2d-54            [-1, 256, 8, 8]          65,792\n  DepthwiseConv2d-55            [-1, 256, 8, 8]               0\n      BatchNorm2d-56            [-1, 256, 8, 8]             512\n             ReLU-57            [-1, 256, 8, 8]               0\n          Dropout-58            [-1, 256, 8, 8]               0\n        AvgPool2d-59            [-1, 256, 1, 1]               0\n           Conv2d-60             [-1, 64, 1, 1]          16,448\n           Conv2d-61             [-1, 10, 1, 1]             650\n================================================================\nTotal params: 209,160\nTrainable params: 209,160\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 16.59\nParams size (MB): 0.80\nEstimated Total Size (MB): 17.40\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(Model().to(device), input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),  # Converting to Tensor\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalizing\n",
    "])\n",
    "\n",
    "train_loader = datasets.cifar10(download=True, batch_size=batch_size, use_default_transforms=True)\n",
    "test_loader = datasets.cifar10(train=False, download=True, batch_size=batch_size, use_default_transforms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# img, labels = dataiter.next()\n",
    "# print(img.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exps = []\n",
    "net = SirNet().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "exp1 = Experiment(\n",
    "    name=\"Sir's Model\",\n",
    "    model=net,\n",
    "    solver_cls=ClassificationSolver,\n",
    "    train_args=dict(\n",
    "        epochs=epochs,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        loss_fn=F.cross_entropy\n",
    "    )\n",
    ")\n",
    "exps.append(exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "# model = tv.models.vgg13(pretrained=False).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.008, momentum=0.95)\n",
    "exp2 = Experiment(\n",
    "    name=\"My Model\",\n",
    "    model=model,\n",
    "    solver_cls=ClassificationSolver,\n",
    "    train_args=dict(\n",
    "        epochs=epochs,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    ")\n",
    "exps.append(exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1m\u001b[92m=> Running experiment: My Model\u001b[0m\n\u001b[1m\u001b[93mWarning:\u001b[0m Loss function not specified. Using nll loss.\nEpoch: 1 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 1.4249 - accuracy: 32.4180\nTest set: Average loss: 2.7876, Accuracy: 1754/10000 (17.54%)\n\nEpoch: 2 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 1.1722 - accuracy: 49.8280\nTest set: Average loss: 3.1927, Accuracy: 1913/10000 (19.13%)\n\nEpoch: 3 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 1.1844 - accuracy: 57.7580\nTest set: Average loss: 2.3994, Accuracy: 2845/10000 (28.45%)\n\nEpoch: 4 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.9429 - accuracy: 62.7520\nTest set: Average loss: 2.4022, Accuracy: 3191/10000 (31.91%)\n\nEpoch: 5 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.9952 - accuracy: 66.2560\nTest set: Average loss: 1.9437, Accuracy: 3875/10000 (38.75%)\n\nEpoch: 6 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.7192 - accuracy: 69.1580\nTest set: Average loss: 1.7407, Accuracy: 4469/10000 (44.69%)\n\nEpoch: 7 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.7209 - accuracy: 71.6560\nTest set: Average loss: 1.2082, Accuracy: 5755/10000 (57.55%)\n\nEpoch: 8 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.8690 - accuracy: 72.6580\nTest set: Average loss: 1.0124, Accuracy: 6389/10000 (63.89%)\n\nEpoch: 9 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.5791 - accuracy: 74.5840\nTest set: Average loss: 1.0071, Accuracy: 6426/10000 (64.26%)\n\nEpoch: 10 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.7633 - accuracy: 75.4540\nTest set: Average loss: 0.9588, Accuracy: 6616/10000 (66.16%)\n\nEpoch: 11 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.6522 - accuracy: 76.6720\nTest set: Average loss: 0.8384, Accuracy: 7026/10000 (70.26%)\n\nEpoch: 12 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.5790 - accuracy: 77.7500\nTest set: Average loss: 0.8456, Accuracy: 7098/10000 (70.98%)\n\nEpoch: 13 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.5626 - accuracy: 78.2400\nTest set: Average loss: 0.7891, Accuracy: 7209/10000 (72.09%)\n\nEpoch: 14 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.6919 - accuracy: 78.9980\nTest set: Average loss: 0.7811, Accuracy: 7287/10000 (72.87%)\n\nEpoch: 15 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.6981 - accuracy: 79.5300\nTest set: Average loss: 0.7026, Accuracy: 7571/10000 (75.71%)\n\nEpoch: 16 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.7127 - accuracy: 80.0860\nTest set: Average loss: 0.6759, Accuracy: 7622/10000 (76.22%)\n\nEpoch: 17 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.4605 - accuracy: 80.4440\nTest set: Average loss: 0.7276, Accuracy: 7444/10000 (74.44%)\n\nEpoch: 18 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.5569 - accuracy: 81.3380\nTest set: Average loss: 0.7126, Accuracy: 7528/10000 (75.28%)\n\nEpoch: 19 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.5058 - accuracy: 81.6220\nTest set: Average loss: 0.6980, Accuracy: 7562/10000 (75.62%)\n\nEpoch: 20 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.5045 - accuracy: 82.0600\nTest set: Average loss: 0.7885, Accuracy: 7299/10000 (72.99%)\n\nEpoch: 21 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.5701 - accuracy: 82.5500\nTest set: Average loss: 0.6287, Accuracy: 7816/10000 (78.16%)\n\nEpoch: 22 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.6105 - accuracy: 82.9580\nTest set: Average loss: 0.5617, Accuracy: 8037/10000 (80.37%)\n\nEpoch: 23 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3692 - accuracy: 83.2740\nTest set: Average loss: 0.6220, Accuracy: 7834/10000 (78.34%)\n\nEpoch: 24 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3893 - accuracy: 83.5360\nTest set: Average loss: 0.6113, Accuracy: 7913/10000 (79.13%)\n\nEpoch: 25 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.5372 - accuracy: 83.7460\nTest set: Average loss: 0.6879, Accuracy: 7704/10000 (77.04%)\n\nEpoch: 26 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.6198 - accuracy: 84.0820\nTest set: Average loss: 0.7102, Accuracy: 7648/10000 (76.48%)\n\nEpoch: 27 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.4395 - accuracy: 84.2200\nTest set: Average loss: 0.5363, Accuracy: 8127/10000 (81.27%)\n\nEpoch: 28 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3443 - accuracy: 84.5200\nTest set: Average loss: 0.6437, Accuracy: 7823/10000 (78.23%)\n\nEpoch: 29 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.6213 - accuracy: 85.0060\nTest set: Average loss: 0.5841, Accuracy: 7991/10000 (79.91%)\n\nEpoch: 30 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3993 - accuracy: 84.9540\nTest set: Average loss: 0.5821, Accuracy: 8010/10000 (80.10%)\n\nEpoch: 31 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.5726 - accuracy: 85.4620\nTest set: Average loss: 0.5398, Accuracy: 8196/10000 (81.96%)\n\nEpoch: 32 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3695 - accuracy: 85.4500\nTest set: Average loss: 0.5603, Accuracy: 8069/10000 (80.69%)\n\nEpoch: 33 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.3686 - accuracy: 85.6440\nTest set: Average loss: 0.5557, Accuracy: 8151/10000 (81.51%)\n\nEpoch: 34 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.2797 - accuracy: 85.7540\nTest set: Average loss: 0.5804, Accuracy: 8046/10000 (80.46%)\n\nEpoch: 35 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.3812 - accuracy: 86.2600\nTest set: Average loss: 0.5859, Accuracy: 8021/10000 (80.21%)\n\nEpoch: 36 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.5709 - accuracy: 86.4420\nTest set: Average loss: 0.5382, Accuracy: 8191/10000 (81.91%)\n\nEpoch: 37 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.4424 - accuracy: 86.6740\nTest set: Average loss: 0.6309, Accuracy: 7896/10000 (78.96%)\n\nEpoch: 38 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.6001 - accuracy: 86.5380\nTest set: Average loss: 0.5838, Accuracy: 8005/10000 (80.05%)\n\nEpoch: 39 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.4244 - accuracy: 87.1160\nTest set: Average loss: 0.5422, Accuracy: 8165/10000 (81.65%)\n\nEpoch: 40 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.4400 - accuracy: 87.3800\nTest set: Average loss: 0.5803, Accuracy: 8077/10000 (80.77%)\n\nEpoch: 41 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3011 - accuracy: 87.2960\nTest set: Average loss: 0.5438, Accuracy: 8193/10000 (81.93%)\n\nEpoch: 42 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.1813 - accuracy: 87.6240\nTest set: Average loss: 0.5227, Accuracy: 8245/10000 (82.45%)\n\nEpoch: 43 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.3429 - accuracy: 87.5540\nTest set: Average loss: 0.6158, Accuracy: 7980/10000 (79.80%)\n\nEpoch: 44 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.4348 - accuracy: 87.9640\nTest set: Average loss: 0.5093, Accuracy: 8304/10000 (83.04%)\n\nEpoch: 45 / 50\n391/391 [==============================] - 24s 60ms/step - loss: 0.3375 - accuracy: 87.8060\nTest set: Average loss: 0.5010, Accuracy: 8328/10000 (83.28%)\n\nEpoch: 46 / 50\n391/391 [==============================] - 23s 60ms/step - loss: 0.3472 - accuracy: 88.2200\nTest set: Average loss: 0.5793, Accuracy: 8096/10000 (80.96%)\n\nEpoch: 47 / 50\n391/391 [==============================] - 24s 62ms/step - loss: 0.3202 - accuracy: 88.1320\nTest set: Average loss: 0.5764, Accuracy: 8108/10000 (81.08%)\n\nEpoch: 48 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.2361 - accuracy: 88.4880\nTest set: Average loss: 0.5314, Accuracy: 8221/10000 (82.21%)\n\nEpoch: 49 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.1522 - accuracy: 88.2600\nTest set: Average loss: 0.5238, Accuracy: 8283/10000 (82.83%)\n\nEpoch: 50 / 50\n391/391 [==============================] - 24s 61ms/step - loss: 0.4329 - accuracy: 88.7100\nTest set: Average loss: 0.5961, Accuracy: 8080/10000 (80.80%)\n\n"
    }
   ],
   "source": [
    "# for e in exps:\n",
    "#     e.run()\n",
    "exp2.run()"
   ]
  }
 ]
}